
chapter3. RDD(Resilient Distributed Dataset)로 프로그래밍하기
탄력성있는 분산 데이터셋

RDD는 단순하게는 분산되어 존재하는 데이터 요소들의 모임
스파크에서 모든 작업은 새로운 RDD를 만들거나 존재하는 RDD를 변형하거나,
결과 계산을위해 RDD에서 연산을 호출하는 것들 중의 하나로 표현된다.
스파크는 자동으로 RDD에 있는 데이터들을 클러스터에 분배하며 클러스터
위에서 수행하는 연산들을 병렬화한다.


- RDD기초
스파크의 RDD는 분산되어있는 변경 불가능한 객체 모음
각 RDD는 클러스터의 서로 다른 노드들에서 연산 가능하도록 여러 개의 파티션으로 나뉜다.
RDD는 사용자 정의 클래스를 포함해 파이썬, 자바, 스칼라의 어떤 타입의 객체든
가질 수 있다.

RDD는 두가지 타입의 연산을 지원
. 트랜스포메이션(transformation)
. 액션(action)

트랜스포메이션은 존재하는 RDD에서 새로운 RDD를 만들어낸다.
>>> lines = sc.textFile("README.md")

// filter() 트랜스포메이션 호출
>>> pythonLines = lines.filter(lambda line: "python" in line)

// first()액션 호출
>>> pythonLines.first()

스파크는 RDD를 lazy evaluation으로 액션을 사용하는 시점에 처리

스파크는 RDD들은 기본적으로 액션이 실행될 때마다 매번 새로연산
재사용을 하고 싶을 때, RDD.persist()를 사용


- RDD생성하기
스파크는 RDD를 만드는 두 가지 방법이 있다.
1. 외부 데이터 로드와, 
val lines = sc.textFile("/path/to/README.md")

2. 직접 만든 드라이버 프로그램에서 데이터 집합을 병렬화하는 방법
var lines = sc.parallelize(List("pandas", "i like pandas"))


- RDD의 연산
두 가지 타입의 연산작업, 트렌스포메이션과 액션을 지원
트랜스포메이션 : 새로운 RDD를 만들어내는 연산 map(), filter()
액션 : 드라이버 프로그램에 결과를 돌려줘나 스토리지에 결과를 써 넣는 연산 count(), first()


트렌스포메이션
새로운 RDD를 만들어 돌려주는 RDD연산 방식
실제로 액션이 사용되는 시점에 계산됨
RDD는 변경 불가능한 것

val inputRDD = sc.textFile("log.txt")
val errorsRDD = inputRDD.filter(line => line.contains("error"))
val warningsRDD = inputRDD.filter(line => line.contains("warning"))
val badLinesRDD = errorsRDD.union(warningsRDD)

println("Input had " + badLinesRDD.count() + " concerning lines")
println("Here are 10 examples:")
badLinesRDD.take(10).foreach(println)


스파크는 각 RDD에 대해 lineage graph라는 그래프를 가지고 있음
이 정보를 필요시 각 RDD를 재연산하거나 저장된 RDD가 유실된 경우 복구하는데
활용된다.



액션
여유로운 수행방식
드라이버에 최종 결과 값을 되덜려주거나 외부 저장소에 값을 기록하는 연산작업
badLinesRDD.take(10).foreach(println)

RDD들은 전체 RDD데이터를 가져올 수 있게 해주는 collect()라는 함수도 지원
하지만 filter()에 의해 작은 크기의 데이터 세트의 RDD로 만든 후,
데이터 처리 권장

새로운 액션을 호출할 때마다 전체 RDD가 처음부터(from scrach)
계산된다는 것으 ㄴ중요한 점이다. 이런것을 피하려면 영속화(persist) 하는 방법을 사용


여유로운 수행 방식
스파크는 액션을 만나기 전까지는 실제로 트렌스포메이션을 처리하지 않는다.
함수형 언어에서는 친숙한 방식
트렌스포메이션을 호출할 때, 스파크가 내부적으로 메타데이터에 어떤 연산이 요청되었는지 만을 기록
실제로는 데이터를 어떻게 계산할지에 대한 명령어들을 가지고 있다.
하둡에서는 복잡한 매핑코드, 스파크에서는 단순한 연산들을 연결


스파크에 함수 전달하기
대부분의 트렌스포메이션과 액션 일부는 스파크가 실제로 
연산에 쓰일 함수들을 전달해야 하는 구조를 가진다.


많이 쓰이는 트랜스포메이션과 액션
데이터 요소 위주 트랜스포메이션
map() -> 함수를 받아 RDD의 각 데이ㅓㅌ에 적용하고 결과 RDD에 각 데이터의 새 결과값을 담는다.
filter() -> 함수를 받아 filter함수를 통과한 데이터만 담는다.
var input = sc.parallelize(List(1,2,3,4))
var result = input.map(x => x * x)
println(result.collect().mkString(","))

flatMap() 각 입력에 대해 여러개의 아웃풋 데이터를 생성해야할 때
var lines = sc.parallelize(List("hello world", "hi"))
var words = lines.flatMap(line => line.split(" "))
words.first()


기상 집합 연산
RDD는 합집합 교집합 같은 다양한 수학적 집합 연산 지원
(distinct, union, interception, subtract, 카테시안 곱)


액션
reduce() 두개의 데이터를 합쳐 같은 타입 데이터 하나를 반호나하는 함수를 받는다.
var sum = rdd.reduce(x,y => x + y)


RDD타입 간 변혼하기
어떤 함수들은 특정한 타입의 RDD에서만 쓸 수 있다.


영속화(캐싱)
동일한 RDD를 여러 번 사용하고 싶을 때도 있을 것이다.
생각없이 이를 시도한다면 스파크는 RDD와 RDD에서 호출하는 액션들에 대한 모든 의존성을 재연산하게 된다.
LRU방식의 캐싱

import org.apache.spark.storage.StorageLevel
var result = input.map(x => x*x)
result.persist(StorageLevel.DISK_ONLY)
println(result.count())
println(result.collect().mkString(","))
영속화 데이터를 저장하는 장소는 JVM heap에 객체로 저장


